
<html lang="en">

<head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-CEKV88TVGH"></script>
    <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-CEKV88TVGH');
    </script>
    
    <title>MoReFree</title>

    <meta name="author" content="Zhao Yang">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <base target="_blank">

    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/css/bootstrap.min.css">
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.4.0/css/font-awesome.min.css">
    <link rel="stylesheet" href="css/bootstrap.min.css">
    <link rel="stylesheet" href="../../stylesheet.css">
    <link rel="stylesheet" href="css/app.css?v=20230203">
    <link rel="icon" type="image/png" sizes="32x32" href="../../images/favicon/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="../../images/favicon/favicon-16x16.png">

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.3/jquery.min.js"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/js/bootstrap.min.js"></script>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

    <script src="js/app.js"></script>
</head>

<body>
    <div class="container" id="main">
        <div class="row">
            <h2 class="col-md-12 text-center">
                <b>MoReFree</b>: <br> World Models Increase Autonomy in Reinforcement Learning</br> 
                <small>
                Arxiv Preprint
                </small>
            </h2>
        </div>
        <div class="row">
            <div class="col-md-12 text-center">
                <ul class="list-inline">
                    <li>
                        <a href="https://yangzhao-666.github.io/">
                          Zhao Yang
                        </a>
                    </li>
                    <li>
                        <a href="https://thomasmoerland.nl">
                        Thomas M. Moerland
                        </a>
                    </li>
                    <li>
                        <a href="https://scholar.google.se/citations?user=KGlyGUcAAAAJ&hl=en">
                          Mike Preuss
                        </a>
                    </li>
                    <li>
                        <a href="https://askeplaat.wordpress.com">
                          Aske Plaat
                        </a>
                    </li>
                    <li>
                        <a href="https://edwardshu.com">
                          Edward S. Hu
                        </a>
                    </li>
                    
                    </br>Leiden University, University of Pennsylvania
                </ul>
            </div>
        </div>

        <div class="row">
                <div class="col-md-4 col-md-offset-4 text-center">
                    <ul class="nav nav-pills nav-justified">
                        <li>
                            <a href="https://arxiv.org/abs/2406.00324">
                            <image src="img/DoDont_paper.png" height="60px">
                                <h4><strong>Paper</strong></h4>
                            </a>
                        </li>
                        <!-- <li>
                            <a href="https://neurips.cc/virtual/2023/poster/72467">
                            <image src="img/youtube_icon.png" height="60px">
                                <h4><strong>Video</strong></h4>
                            </a>
                        </li>
                        <li>
                            <a href="https://github.com/dojeon-ai/DISCO-DANCE">
                            <image src="img/github.png" height="60px">
                                <h4><strong>Code</strong></h4>
                            </a>
                        </li> -->
                    </ul>
                </div>
        </div>

        <div class="row">
            <div class="col-md-12">
                <h2>
                    <span class="font-weight-normal">Abstract</span>
                </h2>
                <br>
                <p class="text-justify">
                    Reinforcement learning (RL) is an appealing paradigm for training intelligent agents, enabling policy acquisition from the agent's own autonomously acquired experience. 
                    However, the training process of RL is far from automatic, requiring extensive human effort to reset the agent and environments.
                    To tackle the challenging reset-free setting, we first demonstrate the superiority of model-based (MB) RL methods in such setting, showing that a straightforward application of MBRL can outperform all the prior state-of-the-art methods while requiring less supervision. 
                    We then identify limitations inherent to this direct extension and propose a solution called model-based reset-free (MoReFree) agent, which further enhances the performance. 
                    MoReFree adapts two key mechanisms, exploration and policy learning, to handle reset-free tasks by prioritizing task-relevant states. 
                    It exhibits superior data-efficiency across various reset-free tasks without access to environmental reward or demonstrations while significantly outperforming privileged baselines that require supervision. 
                    Our findings suggest model-based methods hold significant promise for reducing human effort in RL.
                </p>
            </div>
        </div>

        <div class="row">
            <div class="col-md-12">
                <h2>
                    <span class="font-weight-normal">Motivation</span>
                </h2>
            </div>
            <div class="col-md-12">
                <h3>
                    <span class="font-weight-normal">Would MBRL agents excel in the reset-free RL setting?</span>
                </h3>
            </div>
            <div class="col-md-12">
                <br>
                <!-- <div class="vertical-align">-->
                    
                    <div class="col-md-6">
                    <!-- <div class="col-md-3"> -->
                        <p>
                            <img class="img-responsive center-block" src="img/peg.png">
                        </p>
                    </div>
                    <!-- <div class="col-md-3"> -->
                        <p class="text-justify">
                            As an initial attempt, we first evaluate an unsupervised MBRL agent, out-of-the-box, in a reset-free Ant locomotion task. 
                            The ant is reset to the center of a rectangular arena, and is tasked with navigating to the upper right corner. 
                            The agent is reset only once at the start of training. 
                            The evaluation is episodic - the agent is reset at the start of each evaluation episode.
                            For the MBRL agent, we use PEG \citep{hu2023planning}, which was developed to solve hard exploration tasks in the episodic setting. 
                            As seen in the figure, PEG, out of the box, outperforms prior state-of-the-art, model-free agent, IBC, tailored for the reset-free setting. 
                            On the right, we plot state visitation heatmaps of the agents, where lighter colors correspond to more visitations. 
                            The oracle agent, with access to resets, explores the the ``task-relevant'' area between the initial and top right corner, which is ideal for training a policy that succeeds in episodic evaluation.
                            In contrast, PEG exhaustively explores the entire space, as seen through its uniform heatmap.
                            This leads us to ask: <b>how can MBRL agents acquire more task-relevant data in the reset-free setting to improve its performance?</b>
                        </p>
                <!-- </div>-->
            </div>
        </div>

        <div class="row">
            <div class="col-md-12">
                <h2>
                    <span class="font-weight-normal">Results</span>
                </h2>
            </div>
            <div class="col-md-12">
                <p class="text-justify">
                We examine model-based backbone and MoReFree on 8 challenging reset-free tasks, ranging from locomtion to manipulation tasks.
                </p>
                    <div class="col-md-12">
                        <p>
                            <img class="img-responsive center-block" src="img/envs.png">
                        </p>
                    </div>
                    <div class="col-md-12">
                        <p>
                            <img class="img-responsive" src="img/results.png">
                        </p>
                    </div>
                <p class="text-justify">
                DoDont starts by collecting instruction videos of desirable (Do's) and undesirable behaviors (Don'ts). <br>
                We then train an instruction network which assigns higher values to desirable behaviors and lower values to undesirable ones.
                </p>
            </div>
        </div>

        <div class="row">
            <div class="col-md-12">
                <h2>
                    <span class="font-weight-normal">Objective function of DoDont</span>
                </h2>
            </div>
            <div class="col-md-12">
            <p class="text-justify">
            <h3 class="text-center desktop-only">
                $$\begin{aligned}
                \text{Maximize} \ \ r(s, z, s') = \hat{p}_{\psi}(s, s')(\phi(s') - \phi(s))^\top z \quad
                \text{s.t.} \ \ \|\phi(s) - \phi(s')\|_2 \leq 1.
                \end{aligned}$$
            </h3>
            <p class="text-justify">
                This equation represents the final objective of DoDont. Essentially, this can be interpreted as simply multiplying instruction network \(\hat{p}_{\psi}\) to the original learning objective function of METRA. For a detailed derivation, please refer to the main paper.
            </p>
            </div>
        </div>

        <div class="row">
            <div class="col-md-12">
                <hr>
                <p class="text-justify">
                    The website template was borrowed from <a href="https://mynsng.github.io/dodont/">DoDont</a>.
                </p>
            </div>
        </div>


    
</body>
</html>
