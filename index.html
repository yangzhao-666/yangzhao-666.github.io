<!DOCTYPE HTML>
<html lang="en">
<head>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.1.3/dist/css/bootstrap.min.css" integrity="sha384-MCw98/SFnGE8fJT3GXwEOngsV7Zt27NXFoaoApmYm81iuXoPkFOJwJ8ERdknLPMO" crossorigin="anonymous">
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <meta http-equiv="Cache-Control" content="no-cache, no-store, must-revalidate"/>
  <meta http-equiv="Pragma" content="no-cache"/>
  <meta http-equiv="Expires" content="0"/>

  <title>Zhao Yang</title>

  <meta name="author" content="Zhao Yang">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/thinking_face.png?">

</head>

<body>
  <div class="container">
    <div class="row" style="margin-top: 10px;">
      <div class="col-sm-4 name-column" style="min-width: 266px;">
        <p>
          <name>Zhao Yang</name>
        </p>
      </div>
    </div>
    <div class="row common-rows">

      <div class="col-xs-12 col-sm-8 personal-column">
        <p> 
          I am a postdoc at <a href="https://vu-qda.github.io">VU Amsterdam</a> working with <a href="http://vincent.francois-l.be">Vincent François-Lavet</a>. I received my PhD from Leiden University where I worked with <a href="https://thomasmoerland.nl">Thomas Moerland</a>, <a href="https://scholar.google.se/citations?user=KGlyGUcAAAAJ&hl=en">Mike Preuss</a>, and <a href="https://askeplaat.wordpress.com">Aske Plaat</a>. I got my master degree at Leiden University in 2020, and bachelor degree at BLCU, China, in 2018. 
        </p>
        <p>  
          I am interested in artificial intelligence and how it can support decision-making. To that end, I research reinforcement learning and its applications in both virtual environments (e.g. video games and robitics) and real-world domains (e.g. healthcare and LLMs).
        </p>
        <p>  
          I co-host <a href="https://www.benerl.org/seminar-series">BeNeRL seminar</a> and review. 
        </p>

        </p>
        <p style="text-align:center">
          Contact: z.yang(at)liacs.leidenuniv.nl <br>
          <a href="https://scholar.google.com/citations?user=O3Et_TwAAAAJ&hl">Google Scholar</a> &nbsp|&nbsp
          <a href="https://www.linkedin.com/in/zhaoyang-rl/">LinkedIn</a> &nbsp|&nbsp
          <a href="https://twitter.com/zhaoyang3x6">Twitter</a> &nbsp|&nbsp
          <a href="https://github.com/yangzhao-666">Github</a> &nbsp|&nbsp
          <a href="images/ZhaoYang20250220.pdf">CV</a>
        </p>

      </div>
      <div class="col-xs-12 col-sm-4 personal-column" style="margin-top: 30px">
        <img alt="profile photo" src="images/zy.jpg" class="personal-photo">
      </div>
    </div>
  </div>

  <br>
  <div class="container">
    <div class="row section-heading-rows">
      <h4>Publications</h4>
    </div>

    <div class="row common-rows" style="margin-top: 3%">
        <div class="col-xs-12 custom-col-sm-3 left-column">
          <img src='images/xiper.png' class="paper-images">
      </div>
      <div class="col-xs-12 col-sm-9 right-column">
        <br>
            <papertitle>
                An agent that learns from cross-domain videos
            </papertitle>
        </a>
        <br>
          <strong>Zhao Yang</strong>,
          <a href="https://scholar.google.com/citations?user=GMcfK1MAAAAJ&hl=en">Jacob E. Kooi</a>,
          <a href="https://scholar.google.com/citations?user=FBES9RwAAAAJ&hl=en">Thomas Delliaux</a>,
          <a href="http://vincent.francois-l.be">Vincent François-Lavet</a>
        <br>
        <em>In submission</em>.
        <br>
        <a href="https://sites.google.com/view/xiper">Website (for review)</a>
        <br><br>
        <p style="margin-top: -1%;"> 
          Keywords: Cross-domain imitation learning, Video Prediction, RL
        </p>
      </td>
        </p>
      </div>
    </div>
    
    <div class="row common-rows" style="margin-top: 3%">
        <div class="col-xs-12 custom-col-sm-3 left-column">
          <img src='images/performance.png' class="paper-images">
      </div>
      <div class="col-xs-12 col-sm-9 right-column">
        <br>
            <papertitle>
                Hadamax Encoding: Elevating Performance in Model-Free Atari
            </papertitle>
        </a>
        <br>
          <a href="https://scholar.google.com/citations?user=GMcfK1MAAAAJ&hl=en">Jacob E. Kooi</a>,
          <strong>Zhao Yang</strong>,
          <a href="http://vincent.francois-l.be">Vincent François-Lavet</a>
        <br>
        <em>Advances in Neural Information Processing Systems (NeurIPS), 2025</em>.
        <br>
        <a href="https://arxiv.org/pdf/2505.15345">arxiv</a> | <a href="https://github.com/Jacobkooi/Hadamax">Code</a> | <a href="https://github.com/Jacobkooi/Hadamax">Reviews</a>
        <br><br>
        <p style="margin-top: -1%;"> 
          Keywords: Model-free RL, Atari
        </p>
      </td>
        </p>
      </div>
    </div>
    
    <div class="row common-rows" style="margin-top: 3%">
        <div class="col-xs-12 custom-col-sm-3 left-column">
          <img src='images/fog_loop.gif' class="paper-images">
      </div>
      <div class="col-xs-12 col-sm-9 right-column">
        <br>
            <papertitle>
                Guiding Skill Discovery with Foundation Model
            </papertitle>
        </a>
        <br>
        <strong>Zhao Yang</strong>,
          <a href="https://thomasmoerland.nl">Thomas M. Moerland</a>,
          <a href="https://scholar.google.se/citations?user=KGlyGUcAAAAJ&hl=en">Mike Preuss</a>,
          <a href="https://askeplaat.wordpress.com">Aske Plaat</a>,
          <a href="http://vincent.francois-l.be">Vincent François-Lavet</a>,
          <a href="https://edwardshu.com">Edward S. Hu</a>
        <br>
        <em>In submission</em>.
        <br>
        <a href="https://arxiv.org/pdf/2510.23167">arxiv</a> | <a href="https://sites.google.com/view/iclr-fog">Website (for review)</a>
        <br><br>
        <p style="margin-top: -1%;"> 
          Keywords: Foundation Models, Unsupervised Skill Discovery, RL
        </p>
      </td>
        </p>
      </div>
    </div>

    <div class="row common-rows" style="margin-top: 3%">
        <div class="col-xs-12 custom-col-sm-3 left-column">
          <img src='images/ant.gif' class="paper-images">
      </div>
      <div class="col-xs-12 col-sm-9 right-column">
        <br>
            <papertitle>
                Reset-free Reinforcement Learning with World Models
            </papertitle>
        </a>
        <br>
        <strong>Zhao Yang</strong>,
          <a href="https://thomasmoerland.nl">Thomas M. Moerland</a>,
          <a href="https://scholar.google.se/citations?user=KGlyGUcAAAAJ&hl=en">Mike Preuss</a>,
          <a href="https://askeplaat.wordpress.com">Aske Plaat</a>,
          <a href="https://edwardshu.com">Edward S. Hu</a>
        <br>
        <em>Transactions on Machine Learning Research (TMLR) , 2025</em>.
        <br>
        <a href="https://arxiv.org/pdf/2408.09807">Paper</a> | <a href="https://github.com/yangzhao-666/MoReFree">Code</a> | <a href="https://yangzhao-666.github.io/morefree">Website</a> | <a href="https://openreview.net/forum?id=ZdMIXltJzK&referrer=%5BAuthor%20Console%5D(%2Fgroup%3Fid%3DTMLR%2FAuthors%23your-submissions)">Reviews</a>
        <br><br>
        <p style="margin-top: -1%;"> 
          Keywords: World Models, Reset-free, RL
        </p>
      </td>
        </p>
      </div>
    </div>

    <div class="row common-rows" style="margin-top: 3%">
        <div class="col-xs-12 custom-col-sm-3 left-column">
          <img src='images/asterix.gif' class="paper-images">
      </div>
      <div class="col-xs-12 col-sm-9 right-column">
        <!-- <span style="background-color:#e2c7e5">Reinforcement Learning</span>
        <span style="background-color:#b5ead7">Plasticity</span> -->
        <br>
          <papertitle>
            Two-Memory Reinforcement Learning
          </papertitle>
        </a>
        <br>
          <strong>Zhao Yang</strong>,
          <a href="https://thomasmoerland.nl">Thomas M. Moerland</a>,
          <a href="https://scholar.google.se/citations?user=KGlyGUcAAAAJ&hl=en">Mike Preuss</a>,
          <a href="https://askeplaat.wordpress.com">Aske Plaat</a>
        <br>
        <em>Conference on Games (CoG)</em>, 2023; <em>EWRL</em>, 2023
        <br>
        <a href="https://arxiv.org/pdf/2304.10098.pdf">Paper</a> | <a href="https://github.com/yangzhao-666/2m">Code</a> 
        <br><br>
        <p style="margin-top: -1%;"> 
            Combine episodic control (EC) and RL together. The agent learns to automatically switch between EC and RL.
        </p>
      </div>
    </div>
    
    <div class="row common-rows" style="margin-top: 3%">
    <div class="col-xs-12 custom-col-sm-3 left-column">
        <img src='images/cec_reach_performance.gif' class="paper-images">
    </div>
    <div class="col-xs-12 col-sm-9 right-column">
        <!-- <span style="background-color:#e2c7e5">Reinforcement Learning</span>
        <span style="background-color:#f1f1b2">Adaptation</span> -->
        <br>
        <papertitle>
            Continuous Episodic Control
        </papertitle>
        <br>
        <strong>Zhao Yang</strong>,
          <a href="https://thomasmoerland.nl">Thomas M. Moerland</a>,
          <a href="https://scholar.google.se/citations?user=KGlyGUcAAAAJ&hl=en">Mike Preuss</a>,
          <a href="https://askeplaat.wordpress.com">Aske Plaat</a>
        <br>
        <em>Conference on Games (CoG)</em>, 2023; <em>EWRL</em>, 2023
        <br>
        <a href="https://arxiv.org/pdf/2211.15183.pdf">Paper</a> | <a href="https://github.com/yangzhao-666/cec">Code</a> 
        <br><br>
        <p style="margin-top: -1%;"> 
          Use episodic memory directly for continuous action selection. It outperforms SOTA RL agents.
        </p>
    </div>
    </div>

    <div class="row common-rows" style="margin-top: 3%">
      <div class="col-xs-12 custom-col-sm-3 left-column">
          <img src='images/post-explore.gif' class="paper-images">
      </div>
      <div class="col-xs-12 col-sm-9 right-column">
          <!-- <span style="background-color:#e2c7e5">Reinforcement Learning</span>
          <span style="background-color:#ff9aa2">Skill Discovery</span> -->
          <br>
          <!-- <a href="http://mynsng.github.io/discodance"> -->
          <papertitle>
              First Go, then Post-Explore: the Benefits of Post-Exploration in Intrinsic Motivation
          </papertitle>
          </a>
          <br>
          <strong>Zhao Yang</strong>,
          <a href="https://thomasmoerland.nl">Thomas M. Moerland</a>,
          <a href="https://scholar.google.se/citations?user=KGlyGUcAAAAJ&hl=en">Mike Preuss</a>,
          <a href="https://askeplaat.wordpress.com">Aske Plaat</a>
          <br>
          <em>International Conference on Agents and Artificial Intelligence (ICAART)</em>, 2023; <em>ALOE workshop @ICLR</em>, 2022
          <br>
          <a href="https://arxiv.org/pdf/2212.03251.pdf">Paper</a>
          <br><br>
          <p style="margin-top: -1%;"> 
          Systematically illustrate that why and how Go-Explore works in tabular and deep RL settings. Explore ('exp') can help the agent step into unseen areas.
          </p>
      </div>
      </div>

    <div class="row common-rows" style="margin-top: 3%">
    <div class="col-xs-12 custom-col-sm-3 left-column">
        <img src='images/sokoban.gif' class="paper-images">
    </div>
    <div class="col-xs-12 col-sm-9 right-column">
        <!-- <span style="background-color:#e2c7e5">Reinforcement Learning</span>
        <span style="background-color:#b5ead7">Plasticity</span> -->
        <br>
        <papertitle>
            Transfer Learning and Curriculum Learning in Sokoban
        </papertitle>
        </a>
        <br>
        <strong>Zhao Yang</strong>,
          <a href="https://scholar.google.se/citations?user=KGlyGUcAAAAJ&hl=en">Mike Preuss</a>,
          <a href="https://askeplaat.wordpress.com">Aske Plaat</a>
        <br>
        <em>Benelux Conference on Artificial Intelligence (BNAIC)</em>, 2021
        <br>
        <a href="https://arxiv.org/pdf/2105.11702.pdf">Paper</a> | <a href="https://github.com/yangzhao-666/TLCLS">Code</a>
        <br><br>
        <p style="margin-top: -1%;"> 
            Pre-train and fine-tune neural networks on Sokoban tasks. Agents pre-trained in 1-box tasks can learn faster in 2/3-box tasks, but not vice versa.
        </p>
    </div>
    </div>


  <div class="container">
    <div class="row">
      <div class="col">
        <p style="text-align:right;font-size:small;">
          Template based on <a href="https://mynsng.github.io">Hyunseung's website</a>. Latest update: 05/2025.
        </p>
      </div>
    </div>
  </div>
</body>

</html>
