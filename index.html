<!DOCTYPE HTML>
<html lang="en">
<head>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.1.3/dist/css/bootstrap.min.css" integrity="sha384-MCw98/SFnGE8fJT3GXwEOngsV7Zt27NXFoaoApmYm81iuXoPkFOJwJ8ERdknLPMO" crossorigin="anonymous">
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <meta http-equiv="Cache-Control" content="no-cache, no-store, must-revalidate"/>
  <meta http-equiv="Pragma" content="no-cache"/>
  <meta http-equiv="Expires" content="0"/>

  <title>Zhao Yang</title>

  <meta name="author" content="Zhao Yang">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/thinking_face.png?">

</head>

<body>
  <div class="container">
    <div class="row" style="margin-top: 10px;">
      <div class="col-sm-4 name-column" style="min-width: 266px;">
        <p>
          <name>Zhao Yang</name>
        </p>
      </div>
    </div>
    <div class="row common-rows">

      <div class="col-xs-12 col-sm-8 personal-column">
        <p> 
          I am a PhD student at the <a href="https://rlg.liacs.nl">Reinforcement Learning Group</a>, Leiden University, supervised by <a href="https://thomasmoerland.nl">Thomas Moerland</a>, <a href="https://scholar.google.se/citations?user=KGlyGUcAAAAJ&hl=en">Mike Preuss</a>, and <a href="https://askeplaat.wordpress.com">Aske Plaat</a>. I got my master degree at Leiden University in 2020, and bachelor degree at BLCU, China, in 2018. 
        </p>
        <p>  
          I'm interested in reinforcement learning and trying to automate agents using [intrinsic motivation, world models...], mostly in games and robotic tasks.
        </p>
        <p>  
          I co-host <a href="https://www.benerl.org/seminar-series">BeNeRL seminar</a>, assist courses <a href="https://rl.liacs.nl">DRL</a>, <a href="https://arl.liacs.nl/home">SADRL</a>, <a href="https://studiegids.universiteitleiden.nl/en/courses/114078/video-games-for-research">VG4R</a>, and review. 
        </p>

        </p>
        <p style="text-align:center">
          Contact: z.yang(at)liacs.leidenuniv.nl <br>
          <a href="https://scholar.google.com/citations?user=O3Et_TwAAAAJ&hl">Google Scholar</a> &nbsp|&nbsp
          <a href="https://www.linkedin.com/in/zhaoyang-rl/">LinkedIn</a> &nbsp|&nbsp
          <a href="https://twitter.com/zhaoyang3x6">Twitter</a> &nbsp|&nbsp
          <a href="https://github.com/yangzhao-666">Github</a> &nbsp|&nbsp
          <a href="images/ZhaoYang202408.pdf">CV</a>
        </p>

      </div>
      <div class="col-xs-12 col-sm-4 personal-column" style="margin-top: 30px">
        <img alt="profile photo" src="images/zy.png" class="personal-photo">
      </div>
    </div>
  </div>

  <br>
  <div class="container">
    <div class="row section-heading-rows">
      <h4>Publications</h4>
    </div>

    <div class="row common-rows" style="margin-top: 3%">
        <div class="col-xs-12 custom-col-sm-3 left-column">
          <img src='images/ant.gif' class="paper-images">
      </div>
      <div class="col-xs-12 col-sm-9 right-column">
        <!-- <span style="background-color:#e2c7e5">Reinforcement Learning</span>
        <span style="background-color:#ff9aa2">Skill Discovery</span> -->
        <br>
        <a href="http://mynsng.github.io/dodont"></a>
            <papertitle>
                World Models Increase Autonomy in Reinforcement Learning
            </papertitle>
        </a>
        <br>
        <strong>Zhao Yang</strong>, etc.
        <a href="https://thomasmoerland.nl>Thomas M. Moerland</a>,
          <a href="https://scholar.google.se/citations?user=KGlyGUcAAAAJ&hl=en>Mike Preuss</a>,
          <a href="https://askeplaat.wordpress.com">Aske Plaat</a>
          <a href="https://edwardshu.com">Edward S. Hu</a>
        <br>
        <em>Preprint, in submission</em>.
        <br>
        <a href="https://arxiv.org/pdf/2408.09807">Paper</a> | <a href="https://yangzhao-666.github.io/morefree">Website</a> | <a href="https://sites.google.com/view/morefree">Website (for review)</a>
        <br><br>
        <p style="margin-top: -1%;"> 
          Keywords: World Models, Autonomy, Unsupervised RL
        </p>
      </td>
        </p>
      </div>
    </div>

    <div class="row common-rows" style="margin-top: 3%">
        <div class="col-xs-12 custom-col-sm-3 left-column">
          <img src='images/asterix.gif' class="paper-images">
      </div>
      <div class="col-xs-12 col-sm-9 right-column">
        <!-- <span style="background-color:#e2c7e5">Reinforcement Learning</span>
        <span style="background-color:#b5ead7">Plasticity</span> -->
        <br>
          <papertitle>
            Two-Memory Reinforcement Learning
          </papertitle>
        </a>
        <br>
          <strong>Zhao Yang</strong>,
          <a href="https://thomasmoerland.nl>Thomas M. Moerland</a>,
          <a href="https://scholar.google.se/citations?user=KGlyGUcAAAAJ&hl=en>Mike Preuss</a>,
          <a href="https://askeplaat.wordpress.com">Aske Plaat</a>
        <br>
        <em>COG</em>, 2023; <em>EWRL</em>, 2023
        <br>
        <a href="https://arxiv.org/pdf/2304.10098.pdf">Paper</a> | <a href="https://github.com/yangzhao-666/2m">Code</a> 
        <br><br>
        <p style="margin-top: -1%;"> 
            Combine episodic control (EC) and RL together. The agent learns to automatically switch between EC and RL.
        </p>
      </div>
    </div>
    
    <div class="row common-rows" style="margin-top: 3%">
    <div class="col-xs-12 custom-col-sm-3 left-column">
        <img src='images/cec_reach_performance.gif' class="paper-images">
    </div>
    <div class="col-xs-12 col-sm-9 right-column">
        <!-- <span style="background-color:#e2c7e5">Reinforcement Learning</span>
        <span style="background-color:#f1f1b2">Adaptation</span> -->
        <br>
        <papertitle>
            Continuous Episodic Control
        </papertitle>
        <br>
        <strong>Zhao Yang</strong>,
          <a href="https://thomasmoerland.nl>Thomas M. Moerland</a>,
          <a href="https://scholar.google.se/citations?user=KGlyGUcAAAAJ&hl=en>Mike Preuss</a>,
          <a href="https://askeplaat.wordpress.com">Aske Plaat</a>
        <br>
        <em>COG</em>, 2023; <em>EWRL</em>, 2023
        <br>
        <a href="https://arxiv.org/pdf/2211.15183.pdf">Paper</a> | <a href="https://github.com/yangzhao-666/cec">Code</a> 
        <br><br>
        <p style="margin-top: -1%;"> 
          Use episodic memory directly for continuous action selection. It outperforms SOTA RL agents.
        </p>
    </div>
    </div>

    <div class="row common-rows" style="margin-top: 3%">
      <div class="col-xs-12 custom-col-sm-3 left-column">
          <img src='images/post-explore.gif' class="paper-images">
      </div>
      <div class="col-xs-12 col-sm-9 right-column">
          <!-- <span style="background-color:#e2c7e5">Reinforcement Learning</span>
          <span style="background-color:#ff9aa2">Skill Discovery</span> -->
          <br>
          <!-- <a href="http://mynsng.github.io/discodance"> -->
          <papertitle>
              First Go, then Post-Explore: the Benefits of Post-Exploration in Intrinsic Motivation
          </papertitle>
          </a>
          <br>
          <strong>Zhao Yang</strong>,
          <a href="https://thomasmoerland.nl>Thomas M. Moerland</a>,
          <a href="https://scholar.google.se/citations?user=KGlyGUcAAAAJ&hl=en>Mike Preuss</a>,
          <a href="https://askeplaat.wordpress.com">Aske Plaat</a>
          <br>
          <em>ICAART</em>, 2023; <em>ALOE workshop @ICLR</em>, 2022
          <br>
          <a href="https://arxiv.org/pdf/2212.03251.pdf">Paper</a>
          <br><br>
          <p style="margin-top: -1%;"> 
          Systematically illustrate that why and how Go-Explore works in tabular and deep RL settings. Explore ('exp') can help the agent step into unseen areas.
          </p>
      </div>
      </div>

    <div class="row common-rows" style="margin-top: 3%">
    <div class="col-xs-12 custom-col-sm-3 left-column">
        <img src='images/sokoban.gif' class="paper-images">
    </div>
    <div class="col-xs-12 col-sm-9 right-column">
        <!-- <span style="background-color:#e2c7e5">Reinforcement Learning</span>
        <span style="background-color:#b5ead7">Plasticity</span> -->
        <br>
        <papertitle>
            Transfer Learning and Curriculum Learning in Sokoban
        </papertitle>
        </a>
        <br>
        <strong>Zhao Yang</strong>,
          <a href="https://scholar.google.se/citations?user=KGlyGUcAAAAJ&hl=en>Mike Preuss</a>,
          <a href="https://askeplaat.wordpress.com">Aske Plaat</a>
        <br>
        <em>BNAIC</em>, 2021
        <br>
        <a href="https://arxiv.org/pdf/2105.11702.pdf">Paper</a> | <a href="https://github.com/yangzhao-666/TLCLS">Code</a>
        <br><br>
        <p style="margin-top: -1%;"> 
            Pre-train and fine-tune neural networks on Sokoban tasks. Agents pre-trained in 1-box tasks can learn faster in 2/3-box tasks, but not vice versa.
        </p>
    </div>
    </div>


  <div class="container">
    <div class="row">
      <div class="col">
        <p style="text-align:right;font-size:small;">
          Template based on <a href="https://mynsng.github.io">Hyunseung's website</a>. Latest update: 08/2024.
        </p>
      </div>
    </div>
  </div>
</body>

</html>
